{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7f5cb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from tensorflow import keras\n",
    "from joblib import load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import mutual_info_classif as mi\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, cross_validate\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import river\n",
    "from river import compose\n",
    "from river import evaluate\n",
    "from river import metrics\n",
    "from river import preprocessing\n",
    "from river import stream\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dc557cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = KafkaConsumer(\n",
    "    'ml-raw-dns',\n",
    "    bootstrap_servers=\"kafka:9092\",\n",
    "    auto_offset_reset='earliest',\n",
    "    enable_auto_commit=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7c61dfa-cfb0-4e4e-ad80-ef85268db658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your initial Keras model\n",
    "static_model = load('xgb.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f58d63f-15d9-4365-8bb3-1b8bf0eb73c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the static model\n",
    "dynamic_model = static_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b05b00ed-a61c-49a0-8a48-006312a9a3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the desired window size\n",
    "window_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "568c2747-6d68-48ee-b73c-22d1dfdaf6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables for window processing\n",
    "window_data = []\n",
    "window_size = 1000\n",
    "performance_static = []\n",
    "performance_dynamic = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b35216a-97e3-4a3c-9e8e-e2bc94fdfbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deserialize(data):\n",
    "    \n",
    "    # Decode byte string and split by comma\n",
    "    decoded_data = data.decode('utf-8').strip().split(',')\n",
    "    \n",
    "    # Remove additional characters from the 'Target Attack' field\n",
    "    target_attack = decoded_data[15].strip().strip('\\\\n\"')\n",
    "    \n",
    "    \n",
    "    # Convert to appropriate data types\n",
    "    data_dict = {\n",
    "        'timestamp': decoded_data[0].strip('\"'),\n",
    "        'FQDN_count': int(decoded_data[1]),\n",
    "        'subdomain_length': int(decoded_data[2]),\n",
    "        'upper': int(decoded_data[3]),\n",
    "        'lower': int(decoded_data[4]),\n",
    "        'numeric': int(decoded_data[5]),\n",
    "        'entropy': float(decoded_data[6]),\n",
    "        'special': int(decoded_data[7]),\n",
    "        'labels': int(decoded_data[8]),\n",
    "        'labels_max': int(decoded_data[9]),\n",
    "        'labels_average': float(decoded_data[10]),\n",
    "        'longest_word': decoded_data[11],\n",
    "        'sld': decoded_data[12],\n",
    "        'len': int(decoded_data[13]),\n",
    "        'subdomain': int(decoded_data[14]),\n",
    "        'Target Attack': int(target_attack)\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame([data_dict])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "000c9463-e903-47a0-abf2-0c3432a372d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_with_max(df):\n",
    "    for column in df.columns:\n",
    "        # Check if the column is numerical\n",
    "        if pd.api.types.is_numeric_dtype(df[column]):\n",
    "            max_value = df[column].max()\n",
    "            df[column].fillna(max_value, inplace=True)\n",
    "        else:\n",
    "            # For non-numeric columns, consider using mode (most frequent value) or another method\n",
    "            most_common = df[column].mode().iloc[0]\n",
    "            df[column].fillna(most_common, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2a67a8f-5089-4092-b3a6-9ad4eb61fabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df = df.drop(columns='timestamp')\n",
    "    \n",
    "    df = impute_with_max(df)\n",
    "    \n",
    "    types = df.dtypes\n",
    "    #Indexes of categorical columns\n",
    "    categorical_indexes = (types == 'object')\n",
    "    #Indexes of numerical columns\n",
    "    numerical_indexes = ~categorical_indexes\n",
    "    \n",
    "    columns = df.columns\n",
    "    numerical_columns = columns[numerical_indexes]\n",
    "    categorical_columns = columns[categorical_indexes]\n",
    "    \n",
    "    # For outliers, we'll cap the values at the 1st and 99th percentiles for each numerical feature\n",
    "    for feature in numerical_columns:\n",
    "        # Calculate the 1st and 99th percentiles\n",
    "        lower_bound, upper_bound = df[feature].quantile([0.01, 0.99]).values\n",
    "        # Cap the outliers\n",
    "        df[feature] = np.clip(df[feature], lower_bound, upper_bound)\n",
    "        \n",
    "    # Length of the second-level domain\n",
    "    df['sld_length'] = df['sld'].apply(len)\n",
    "    # Presence of numeric characters in the second-level domain\n",
    "    df['sld_numeric'] = df['sld'].str.contains('\\d').astype(int)\n",
    "    \n",
    "    # Length of the longest word\n",
    "    df['longest_word_length'] = df['longest_word'].apply(len)\n",
    "    # Presence of numeric characters in the longest word\n",
    "    df['longest_word_numeric'] = df['longest_word'].str.contains('\\d').astype(int)\n",
    "    \n",
    "    n_features = 2**4\n",
    "    # Initialize the FeatureHasher\n",
    "    hasher_longest_word = FeatureHasher(n_features=n_features, input_type='string')\n",
    "    hasher_sld = FeatureHasher(n_features=n_features, input_type='string')\n",
    "    words_longest_word = df['longest_word'].astype(str).map(lambda x: x.split())\n",
    "    words_sld = df['sld'].astype(str).map(lambda x: x.split())\n",
    "    hashed_features_longest_word = hasher_longest_word.transform(words_longest_word)\n",
    "    hashed_features_sld = hasher_sld.transform(words_sld)\n",
    "    hashed_longest_word_df = pd.DataFrame(hashed_features_longest_word.toarray(),columns=[f'lw_hash_{i}' for i in range(hashed_features_longest_word.shape[1])])\n",
    "    hashed_sld_df = pd.DataFrame(hashed_features_sld.toarray(), columns=[f'sld_hash_{i}' for i in range(hashed_features_sld.shape[1])])\n",
    "    \n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    hashed_sld_df.reset_index(drop=True, inplace=True)\n",
    "    hashed_longest_word_df.reset_index(drop=True, inplace=True)\n",
    "    df = pd.concat([df, hashed_sld_df, hashed_longest_word_df], axis=1)\n",
    "    \n",
    "    df = df.drop(['longest_word', 'sld'], axis=1)\n",
    "    features = ['FQDN_count', 'subdomain_length', 'lower', 'numeric', 'entropy',\n",
    "       'special', 'labels', 'labels_max', 'labels_average', 'sld_length',\n",
    "       'sld_numeric', 'longest_word_length', 'longest_word_numeric']\n",
    "    \n",
    "    y = df['Target Attack']\n",
    "    X = df[features]\n",
    "    \n",
    "    \n",
    "    return X, y\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5628ae65-19d2-4e87-a5c7-3583200add32",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2680210-8a2a-407d-a056-7a20577ff018",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skmultiflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskmultiflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvaluateHoldout\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskmultiflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataStream\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skmultiflow'"
     ]
    }
   ],
   "source": [
    "from skmultiflow.evaluation import EvaluateHoldout\n",
    "from skmultiflow.data import DataStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff0ab66a-26b0-4d1c-9c92-dfd5d660b09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement skmultiflow (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for skmultiflow\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install skmultiflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93c40ae0-54e2-4bc5-977f-d9fa34387dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window Static Model Accuracy: 0.03214285714285714\n",
      "Window Dynamic Model Accuracy: 0.03214285714285714\n"
     ]
    }
   ],
   "source": [
    "for message in consumer:\n",
    "    # Deserialize your message here\n",
    "    df = deserialize(message.value) \n",
    "\n",
    "    # Add to window data\n",
    "    window_data.append(df)\n",
    "\n",
    "    # Check if the window is filled\n",
    "    if len(window_data) >= window_size:\n",
    "        # Convert window data to a suitable format for model input\n",
    "        window_df = pd.concat(window_data, ignore_index=True)\n",
    "        \n",
    "        X , y = preprocess(window_df)\n",
    "        X = scale.fit_transform(X)\n",
    "        \n",
    "        # Predict and evaluate for static model\n",
    "        \n",
    "        predictions_static = static_model.predict_proba(X)\n",
    "        threshold = 0.65\n",
    "        y_pred_static = (predictions_static[:, 1] >= threshold).astype(int)\n",
    "        accuracy_static = f1_score(y, y_pred_static)\n",
    "        performance_static.append(accuracy_static)\n",
    "\n",
    "        # Predict and evaluate for dynamic model\n",
    "        predictions_dynamic = dynamic_model.predict_proba(X)\n",
    "        y_pred_dynamic = (predictions_dynamic[:, 1] >= threshold).astype(int)\n",
    "        accuracy_dynamic = f1_score(y, y_pred_static)\n",
    "        performance_dynamic.append(accuracy_dynamic)\n",
    "\n",
    "        # Output the window's performance\n",
    "        print(f\"Window Static Model Accuracy: {accuracy_static}\")\n",
    "        print(f\"Window Dynamic Model Accuracy: {accuracy_dynamic}\")\n",
    "        \n",
    "\n",
    "        # Decide if retraining is necessary for dynamic model\n",
    "        if should_retrain(performance_dynamic):  # Implement this function based on your criteria\n",
    "            print(\"Retraining dynamic model...\")\n",
    "            dynamic_model.fit(window_data_array, window_labels_array)\n",
    "            print(\"Dynamic model retrained.\")\n",
    "\n",
    "        # Reset for the next window\n",
    "        window_data = []\n",
    "        window_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6c36bb-ad9c-4528-9ef6-8c3cf402ce2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
